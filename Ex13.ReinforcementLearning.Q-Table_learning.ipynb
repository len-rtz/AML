{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b9bdaa1f",
      "metadata": {
        "id": "b9bdaa1f"
      },
      "source": [
        "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c621fd",
      "metadata": {
        "id": "25c621fd"
      },
      "source": [
        "# 13th exercise: <font color=\"#C70039\">First Reinforcement Learning Q-Table learning</font>\n",
        "* Course: AML\n",
        "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
        "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
        "* Editor of notebook: Lena Pickartz\n",
        "* Date: 17.01.2025\n",
        "\n",
        "---------------------------------\n",
        "**GENERAL NOTE 1**:\n",
        "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole.\n",
        "\n",
        "**GENERAL NOTE 2**:\n",
        "* Please, when commenting source code, just use English language only.\n",
        "* When describing an observation please use English language, too.\n",
        "* This applies to all exercises throughout this course.\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "### <font color=\"FFC300\">TASKS</font>:\n",
        "The tasks that you need to work on within this notebook are always indicated below as bullet points.\n",
        "If a task is more challenging and consists of several steps, this is indicated as well.\n",
        "Make sure you have worked down the task list and commented your doings.\n",
        "This should be done by using markdown.<br>\n",
        "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
        "\n",
        "**YOUR TASKS in this exercise are as follows**:\n",
        "1. import the notebook to Google Colab or use your local machine.\n",
        "2. make sure you specified you name and your matriculation number in the header below my name and date.\n",
        "    * set the date too and remove mine.\n",
        "3. read the entire notebook carefully\n",
        "    * add comments whereever you feel it necessary for better understanding\n",
        "    * run the notebook for the first time.\n",
        "4. play with all hyperparameters including the actions, states, rewards table.\n",
        "5. add and implement an ϵ-greedy strategy\n",
        "---------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8d8b5efe-2b35-4935-af30-8c270bd3b9a3",
      "metadata": {
        "tags": [],
        "id": "8d8b5efe-2b35-4935-af30-8c270bd3b9a3"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d567ab5-06ac-418f-96ca-892695bf9f61",
      "metadata": {
        "id": "5d567ab5-06ac-418f-96ca-892695bf9f61"
      },
      "source": [
        "### Create the possible states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9d53fc92-527b-46c6-bc57-4e68423aa61a",
      "metadata": {
        "tags": [],
        "id": "9d53fc92-527b-46c6-bc57-4e68423aa61a"
      },
      "outputs": [],
      "source": [
        "location_to_state = {\n",
        "    'L1' : 0,\n",
        "    'L2' : 1,\n",
        "    'L3' : 2,\n",
        "    'L4' : 3,\n",
        "    'L5' : 4,\n",
        "    'L6' : 5,\n",
        "    'L7' : 6,\n",
        "    'L8' : 7,\n",
        "    'L9' : 8\n",
        "}\n",
        "\n",
        "state_to_location = dict((state,location) for location, state in location_to_state.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ba3fd99",
      "metadata": {
        "id": "9ba3fd99"
      },
      "source": [
        "### Create the actions & rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "349627df",
      "metadata": {
        "tags": [],
        "id": "349627df"
      },
      "outputs": [],
      "source": [
        "actions = [0,1,2,3,4,5,6,7,8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ac22125",
      "metadata": {
        "tags": [],
        "id": "9ac22125"
      },
      "outputs": [],
      "source": [
        "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
        "                   [1,0,1,0,1,0,0,0,0],\n",
        "                   [0,1,0,0,0,1,0,0,0],\n",
        "                   [0,0,0,0,0,0,1,0,0],\n",
        "                   [0,1,0,0,0,0,0,1,0],\n",
        "                   [0,0,1,0,0,0,0,0,0],\n",
        "                   [0,0,0,1,0,0,0,1,0],\n",
        "                   [0,0,0,0,1,0,1,0,1],\n",
        "                   [0,0,0,0,0,0,0,1,0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7828a644-6339-4f56-8066-9dd3d629190a",
      "metadata": {
        "id": "7828a644-6339-4f56-8066-9dd3d629190a"
      },
      "source": [
        "### Def remaining hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "20f112bf-e2e1-42a1-a9b3-df514af058c3",
      "metadata": {
        "tags": [],
        "id": "20f112bf-e2e1-42a1-a9b3-df514af058c3"
      },
      "outputs": [],
      "source": [
        "# initialize the parameters\n",
        "gamma = 0.99 # discount factor\n",
        "alpha = 0.7  # learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37973f00-e730-4fd2-b660-0ff268984e61",
      "metadata": {
        "id": "37973f00-e730-4fd2-b660-0ff268984e61"
      },
      "source": [
        "### Define agent and its attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "36a8c7ba-023b-414c-b007-64e789dcd9f0",
      "metadata": {
        "tags": [],
        "id": "36a8c7ba-023b-414c-b007-64e789dcd9f0"
      },
      "outputs": [],
      "source": [
        "class QAgent():\n",
        "    # initialize everything\n",
        "    def __init__(self, alpha, gamma, location_to_state, actions, rewards, state_to_location):\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.location_to_state = location_to_state\n",
        "        self.actions = actions\n",
        "        self.rewards = rewards\n",
        "        self.state_to_location = state_to_location\n",
        "\n",
        "        # remember, the Q-value table is of size all actions x all states\n",
        "        M = len(location_to_state)\n",
        "        self.Q = np.zeros((M,M), dtype = None, order = 'C')\n",
        "\n",
        "    # now, implement the training method for the agent\n",
        "    def training(self, start_location, end_location, iterations):\n",
        "\n",
        "        rewards_new = np.copy(self.rewards)\n",
        "\n",
        "        ending_state = self.location_to_state[end_location]\n",
        "\n",
        "        rewards_new[ending_state, ending_state] = 999\n",
        "\n",
        "        # DEBUG\n",
        "        print(rewards_new)\n",
        "\n",
        "        # pick random current state\n",
        "        # iterations = the # of training cycles\n",
        "        for i in range(iterations):\n",
        "            current_state = np.random.randint(0,9)\n",
        "            playable_actions = []\n",
        "\n",
        "            # iterate thru the rewards matrix to get states\n",
        "            # that are really reachable from the randomly chosen\n",
        "            # state and assign only those in a list since they are really playable\n",
        "            for j in range(9):\n",
        "                if rewards_new[current_state, j] > 0:\n",
        "                    playable_actions.append(j)\n",
        "\n",
        "            # choosing next random state\n",
        "            # however, make sure that playable_actions is not empty\n",
        "            if len(playable_actions) != 0:\n",
        "                # Randomly select a value from playable_actions\n",
        "                next_state = np.random.choice(playable_actions)\n",
        "\n",
        "            # finding the difference in Q, often referred to as temporal difference\n",
        "            # by means of the Bellman's equation (compare with slides)\n",
        "            TD = rewards_new[current_state, next_state] + self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state, next_state]\n",
        "            self.Q[current_state, next_state] += self.alpha*TD\n",
        "            # DEBUG\n",
        "            #print(f\"Q[{current_state}, {next_state}]:\", self.Q[current_state, next_state])\n",
        "\n",
        "        route = [start_location]\n",
        "        next_location = start_location\n",
        "\n",
        "        # print the optimal route from start to end\n",
        "        self.get_optimal_route(start_location, end_location, next_location, route, self.Q)\n",
        "\n",
        "    # compute the optimal route\n",
        "    def get_optimal_route(self, start_location, end_location, next_location, route, Q):\n",
        "        while(next_location != end_location):\n",
        "            starting_state = self.location_to_state[start_location]\n",
        "            next_state = np.argmax(Q[starting_state,])\n",
        "            next_location = self.state_to_location[next_state]\n",
        "            route.append(next_location)\n",
        "            start_location = next_location\n",
        "        # DEBUG\n",
        "        print('Q-table:',Q)\n",
        "        print(\"optimal route:\", route)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "25bc2a1b-4291-4560-ba23-0dc145e01154",
      "metadata": {
        "tags": [],
        "id": "25bc2a1b-4291-4560-ba23-0dc145e01154",
        "outputId": "3c191762-a6d0-43b8-e547-3228b51f7121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   1   0   0   0   0   0   0   0]\n",
            " [  1   0   1   0   1   0   0   0   0]\n",
            " [  0   1   0   0   0   1   0   0   0]\n",
            " [  0   0   0 999   0   0   1   0   0]\n",
            " [  0   1   0   0   0   0   0   1   0]\n",
            " [  0   0   1   0   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0   0   1   0]\n",
            " [  0   0   0   0   1   0   1   0   1]\n",
            " [  0   0   0   0   0   0   0   1   0]]\n",
            "Q-table: [[    0.         24529.75520626     0.             0.\n",
            "      0.             0.             0.             0.\n",
            "      0.        ]\n",
            " [23062.73080131     0.         23085.68906258     0.\n",
            "  25299.20493687     0.             0.             0.\n",
            "      0.        ]\n",
            " [    0.         23328.48652325     0.             0.\n",
            "      0.         22791.6140084      0.             0.\n",
            "      0.        ]\n",
            " [    0.             0.             0.         31054.48413566\n",
            "      0.             0.         28572.31004584     0.\n",
            "      0.        ]\n",
            " [    0.         24531.63211626     0.             0.\n",
            "      0.             0.             0.         26305.21559313\n",
            "      0.        ]\n",
            " [    0.             0.         23095.5928693      0.\n",
            "      0.             0.             0.             0.\n",
            "      0.        ]\n",
            " [    0.             0.             0.         30189.36315298\n",
            "      0.             0.             0.         26257.64284871\n",
            "      0.        ]\n",
            " [    0.             0.             0.             0.\n",
            "  25667.3577743      0.         28588.77060372     0.\n",
            "  25293.63572661]\n",
            " [    0.             0.             0.             0.\n",
            "      0.             0.             0.         26305.22090234\n",
            "      0.        ]]\n",
            "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
          ]
        }
      ],
      "source": [
        "qagent = QAgent(alpha, gamma, location_to_state, actions, rewards, state_to_location)\n",
        "qagent.training('L9', 'L4', 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play Around with Hyperparameters"
      ],
      "metadata": {
        "id": "8sImkiR59-tp"
      },
      "id": "8sImkiR59-tp"
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the parameters\n",
        "gamma = 0.01 # discount factor\n",
        "alpha = 0.99  # learning rate"
      ],
      "metadata": {
        "id": "4WV5g7gQ99nb"
      },
      "id": "4WV5g7gQ99nb",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qagent = QAgent(alpha, gamma, location_to_state, actions, rewards, state_to_location)\n",
        "qagent.training('L9', 'L4', 1000)"
      ],
      "metadata": {
        "id": "qb5aHJE0-GNU",
        "outputId": "f6ad35bf-33f0-413c-98fc-24e1ba9b30a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qb5aHJE0-GNU",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   1   0   0   0   0   0   0   0]\n",
            " [  1   0   1   0   1   0   0   0   0]\n",
            " [  0   1   0   0   0   1   0   0   0]\n",
            " [  0   0   0 999   0   0   1   0   0]\n",
            " [  0   1   0   0   0   0   0   1   0]\n",
            " [  0   0   1   0   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0   0   1   0]\n",
            " [  0   0   0   0   1   0   1   0   1]\n",
            " [  0   0   0   0   0   0   0   1   0]]\n",
            "Q-table: [[   0.            1.01010111    0.            0.            0.\n",
            "     0.            0.            0.            0.        ]\n",
            " [   1.01010101    0.            1.01010101    0.            1.01011109\n",
            "     0.            0.            0.            0.        ]\n",
            " [   0.            1.01010111    0.            0.            0.\n",
            "     1.01010101    0.            0.            0.        ]\n",
            " [   0.            0.            0.         1009.09090909    0.\n",
            "     0.            1.11090909    0.            0.        ]\n",
            " [   0.            1.01010111    0.            0.            0.\n",
            "     0.            0.            1.01110909    0.        ]\n",
            " [   0.            0.            1.01010101    0.            0.\n",
            "     0.            0.            0.            0.        ]\n",
            " [   0.            0.            0.           11.09090909    0.\n",
            "     0.            0.            1.01110909    0.        ]\n",
            " [   0.            0.            0.            0.            1.01011109\n",
            "     0.            1.11090909    0.            1.01011109]\n",
            " [   0.            0.            0.            0.            0.\n",
            "     0.            0.            1.01110909    0.        ]]\n",
            "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the parameters\n",
        "gamma = 0.99 # discount factor\n",
        "alpha = 0.01  # learning rate"
      ],
      "metadata": {
        "id": "Z2zu6LPE-Kus"
      },
      "id": "Z2zu6LPE-Kus",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qagent = QAgent(alpha, gamma, location_to_state, actions, rewards, state_to_location)\n",
        "qagent.training('L9', 'L4', 1000)"
      ],
      "metadata": {
        "id": "deJysiHG-Pww",
        "outputId": "4be5253b-7c52-468e-e8e9-9a49c5f58da4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "deJysiHG-Pww",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   1   0   0   0   0   0   0   0]\n",
            " [  1   0   1   0   1   0   0   0   0]\n",
            " [  0   1   0   0   0   1   0   0   0]\n",
            " [  0   0   0 999   0   0   1   0   0]\n",
            " [  0   1   0   0   0   0   0   1   0]\n",
            " [  0   0   1   0   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0   0   1   0]\n",
            " [  0   0   0   0   1   0   1   0   1]\n",
            " [  0   0   0   0   0   0   0   1   0]]\n",
            "Q-table: [[0.00000000e+00 8.61980747e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [5.16567549e-01 0.00000000e+00 4.31508732e-01 0.00000000e+00\n",
            "  5.61988418e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 5.26067790e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 6.22283383e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.08218353e+02\n",
            "  0.00000000e+00 0.00000000e+00 1.85805343e+01 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 5.18715155e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.55418515e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 9.18612956e-01 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.31165116e+02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.29158496e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  4.60312590e-01 0.00000000e+00 1.31442610e+01 0.00000000e+00\n",
            "  6.25365782e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.89626522e+00\n",
            "  0.00000000e+00]]\n",
            "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The high learning rate in Run 1 led to more uniform Q-values across non-goal states and has quicker convergence to similar values.\n",
        "\n",
        "The high discount factor in Run 2 resulted in more varied Q-values, a better differentiation between paths and astronger value propagation from the goal state.\n",
        "\n",
        "The reward structure (shown in the adjacency matrix) has a high reward of 999 at the goal state (L4), which both configurations successfully learned to reach."
      ],
      "metadata": {
        "id": "PaTlgpHS-iiJ"
      },
      "id": "PaTlgpHS-iiJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ϵ-greedy strategy"
      ],
      "metadata": {
        "id": "BaQywvmk_JPF"
      },
      "id": "BaQywvmk_JPF"
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent():\n",
        "    def __init__(self, alpha, gamma, epsilon, location_to_state, actions, rewards, state_to_location):\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.location_to_state = location_to_state\n",
        "        self.actions = actions\n",
        "        self.rewards = rewards\n",
        "        self.state_to_location = state_to_location\n",
        "        self.states = list(self.location_to_state.values())\n",
        "        self.Q = np.zeros((len(self.states), len(self.states)))\n",
        "\n",
        "    def get_available_actions(self, state):\n",
        "        \"\"\"Helper function to get valid actions for a state\"\"\"\n",
        "        return [i for i in range(len(self.states)) if self.rewards[state, i] > 0]\n",
        "\n",
        "    def choose_action(self, state, available_actions):\n",
        "        \"\"\"Choose action using ε-greedy strategy\"\"\"\n",
        "        if not available_actions:\n",
        "            print(f\"Warning: No available actions from state {state}\")\n",
        "            return None\n",
        "\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(available_actions)\n",
        "        else:\n",
        "            state_q_values = self.Q[state][available_actions]\n",
        "            return available_actions[np.argmax(state_q_values)]\n",
        "\n",
        "    def training(self, start_location, end_location, iterations):\n",
        "        rewards_new = np.copy(self.rewards)\n",
        "        ending_state = self.location_to_state[end_location]\n",
        "        start_state = self.location_to_state[start_location]\n",
        "\n",
        "        # Set high reward for reaching goal\n",
        "        for i in range(len(self.states)):\n",
        "            if rewards_new[i, ending_state] > 0:\n",
        "                rewards_new[i, ending_state] = 999\n",
        "\n",
        "        for episode in range(iterations):\n",
        "            if episode % 100 == 0:\n",
        "                print(f\"Episode {episode}/{iterations}\")\n",
        "\n",
        "            current_state = start_state\n",
        "            steps = 0\n",
        "            path = [current_state]\n",
        "\n",
        "            while current_state != ending_state and steps < 20:  # Limit steps per episode\n",
        "                available_actions = self.get_available_actions(current_state)\n",
        "\n",
        "                if not available_actions:\n",
        "                    print(f\"Dead end at state {current_state}\")\n",
        "                    break\n",
        "\n",
        "                action = self.choose_action(current_state, available_actions)\n",
        "                if action is None:\n",
        "                    break\n",
        "\n",
        "                # Get reward and update Q-value\n",
        "                reward = rewards_new[current_state, action]\n",
        "                next_available_actions = self.get_available_actions(action)\n",
        "\n",
        "                if next_available_actions:\n",
        "                    next_max_q = np.max(self.Q[action][next_available_actions])\n",
        "                else:\n",
        "                    next_max_q = 0\n",
        "\n",
        "                # Q-learning update\n",
        "                self.Q[current_state, action] = (1 - self.alpha) * self.Q[current_state, action] + \\\n",
        "                                              self.alpha * (reward + self.gamma * next_max_q)\n",
        "\n",
        "                current_state = action\n",
        "                path.append(current_state)\n",
        "                steps += 1\n",
        "\n",
        "        # Get optimal route\n",
        "        route = self.get_optimal_route(start_location, end_location)\n",
        "        print(\"Q-table:\")\n",
        "        print(self.Q)\n",
        "        print(\"Final route:\", route)\n",
        "        return route\n",
        "\n",
        "    def get_optimal_route(self, start_location, end_location):\n",
        "        route = [start_location]\n",
        "        current_location = start_location\n",
        "\n",
        "        while current_location != end_location:\n",
        "            current_state = self.location_to_state[current_location]\n",
        "            available_actions = self.get_available_actions(current_state)\n",
        "\n",
        "            if not available_actions:\n",
        "                print(f\"No valid actions from {current_location}\")\n",
        "                break\n",
        "\n",
        "            next_state = available_actions[np.argmax(self.Q[current_state][available_actions])]\n",
        "            current_location = self.state_to_location[next_state]\n",
        "            route.append(current_location)\n",
        "\n",
        "            if len(route) > 20:  # Prevent infinite loops\n",
        "                print(\"Route finding exceeded maximum length\")\n",
        "                break\n",
        "\n",
        "        return route"
      ],
      "metadata": {
        "id": "XejyIkUC_NBA"
      },
      "id": "XejyIkUC_NBA",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the parameters\n",
        "gamma = 0.99  # discount factor\n",
        "alpha = 0.7  # learning rate\n",
        "epsilon = 0.1  # exploration rate (10% exploration)"
      ],
      "metadata": {
        "id": "17yU2vFaBgP1"
      },
      "id": "17yU2vFaBgP1",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create agent with epsilon parameter added\n",
        "qagent = QAgent(alpha, gamma, epsilon, location_to_state, actions, rewards, state_to_location)\n",
        "qagent.training('L9', 'L4', 1000)"
      ],
      "metadata": {
        "id": "nVUDIVwcBkPm",
        "outputId": "eeb4cd7c-8d97-4587-8aa1-83d32fb9a2ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nVUDIVwcBkPm",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/1000\n",
            "Episode 100/1000\n",
            "Episode 200/1000\n",
            "Episode 300/1000\n",
            "Episode 400/1000\n",
            "Episode 500/1000\n",
            "Episode 600/1000\n",
            "Episode 700/1000\n",
            "Episode 800/1000\n",
            "Episode 900/1000\n",
            "Q-table:\n",
            "[[  0.          96.51164373   0.           0.           0.\n",
            "    0.           0.           0.           0.        ]\n",
            " [ 96.54169024   0.          92.0383249    0.         957.95082072\n",
            "    0.           0.           0.           0.        ]\n",
            " [  0.          94.1851596    0.           0.           0.\n",
            "    0.           0.           0.           0.        ]\n",
            " [  0.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.        ]\n",
            " [  0.         855.00116443   0.           0.           0.\n",
            "    0.           0.         981.10988665   0.        ]\n",
            " [  0.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.        ]\n",
            " [  0.           0.           0.         999.           0.\n",
            "    0.           0.         981.1099       0.        ]\n",
            " [  0.           0.           0.           0.         972.29832729\n",
            "    0.         990.01         0.         972.298801  ]\n",
            " [  0.           0.           0.           0.           0.\n",
            "    0.           0.         981.1099       0.        ]]\n",
            "Final route: ['L9', 'L8', 'L7', 'L4']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L9', 'L8', 'L7', 'L4']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oberservations\n",
        "\n",
        "- Multiple paths were explored (shown by various non-zero Q-values)\n",
        "- Clear convergence to optimal path (shown by very high values ~957-999)\n",
        "- Some sub-optimal paths were explored but got lower values\n",
        "- Found optimal route: L9 → L8 → L7 → L4\n",
        "- High Q-values (~980-999) along this path show strong confidence\n",
        "- 10% exploration rate was enough to find and verify this was the best path"
      ],
      "metadata": {
        "id": "m5IJdjRSKtHz"
      },
      "id": "m5IJdjRSKtHz"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hW2oBQKGK43u"
      },
      "id": "hW2oBQKGK43u",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}